\section{Experimental Setup}
The main question this work aims to answer is: can pre-training with temporal order verification improve action recognition recognition performance from raw RGB inputs without explicit calculation of optical flow in a single stream 3D convolutional neural network.
Specifically we try to develop a temporal order verification pre-training method for 3D CNNs, similar to the original approach in \cite{misra_shuffle_2016}.
This section describes the overall details of the implementation of the \textit{C3D} model \cite{tran_learning_2015}.


\subsection{Hardware Setup}
\label{sec:hardware}
We perform experiments on a single PC with two GPUs.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l X}
Processor & Intel Core i7 950 with 3.07 Ghz clock speed \\
Cores / Threads & 4 / 8 (Hyperthreading) \\
Main memory & 12 Gb DDR3 RAM with 1067Mhz\\
Mass storage & 1 Tb (HDD) \\
GPU & $2\times$ Nvidia GeForce GTX 1070 \\
GPU Memory & $2\times$ 8Gb\\
\end{tabularx}
\caption{Overview hardware setup}
\label{tab:hardware-setup}
\end{table}


\subsection{Model Architecture}
Our implementation of the \textit{C3D} model follows the architectural design of the original \textit{C3D} network, as described in \cite{tran_learning_2015} and in the published Caffe implementation (see \mbox{\url{https://github.com/facebook/C3D}}).
We implemented our model in Python using Google's deep learning framework \textit{Tensorflow} \cite{abadi_tensorflow:_2016}.
The \textit{C3D} network contains eight 3D convolutional layers, with max-pooling layers after the first, second, fourth, sixth and eighth layer.
An illustration of the network model is given in figure \ref{fig:c3d_architecture}.
After the last pooling layer, the temporal information is fully collapsed and the feature maps two-dimensional.
These are then flattened into a one-dimensional vector to be further processed by the fully-connected layers.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_related/c3d_architecture}
    \caption{Illustration of the C3D architecture. An additional flattening operation is performed after \textit{Pool5}, which is not illustrated in this figure. \cite{tran_learning_2015}}
    \label{fig:c3d_architecture}
\end{figure}


\subsubsection{Architectural Details}
According to the experiments conducted by \textcite{tran_learning_2015}, 3D convolutional kernels of size $3 \times 3 \times 3$ perform best in the \textit{C3D} model.
We therefore follow this design decision in our implementation as well.
The network processes inputs of spatial dimension $112 \times 112$ pixels and temporal length of $16$ frames in $3$ RGB channels.
The dimension of a complete input volume therefore is $16 \times 112 \times 112 \times 3$.
The 3D filters in the convolutional layers \textit{conv1a} - \textit{conv5b} are applied to their inputs with stride 1 and zero-padding is applied.
The convolutional layers therefore do not reduce the dimensionality of the inputs in neither spatial nor temporal dimension.

All pooling layers, except the first one, perform $2 \times 2 \times 2$ (\textit{width $\times$ height $\times$ temporal depth}) 3D max-pooling with strides two.
The pooling layers thereby reduce the spatial and temporal dimension of an input by $2$.
The first pooling layer only performs spatial pooling, i.e. with a pooling kernel of dimension $2 \times 2 \times 1$, in order to not collapse the temporal information too early as done in \cite{tran_learning_2015}.
Table \ref{tab:num_params} holds the output dimensions of each layer along with the number of trainable weights and biases.

\begin{table}[H]
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{l X l}
    Layer Name & Output Dimension & Trainable Parameters \\
    \hline
    \textbf{Conv1a} & $16 \times 112 \times 112 \times 64$ & $3 \cdot 3 \cdot 3 \cdot 3 \cdot 64 + 64 = 5,248$\\
    \textbf{Pool1} & $16 \times 112 \times 112 \times 64$ & 0 \\
    \textbf{Conv2a} & $16 \times 56 \times 56 \times 128$ & $64 \cdot 3 \cdot 3 \cdot 3 \cdot 128 + 128 = 221,312$\\
    \textbf{Pool2}& $8 \times 28 \times 28 \times 128$ & 0 \\
    \textbf{Conv3a} & $8 \times 28 \times 28 \times 256$ & $128 \cdot 3 \cdot 3 \cdot 3 \cdot 256 + 256 = 884,992$ \\
    \textbf{Pool3} & $4 \times 14 \times 14 \times 256$ & 0 \\
    \textbf{Conv4a} & $4 \times 14 \times 14 \times 512$ & $256 \cdot 3 \cdot 3 \cdot 3 \cdot 512 + 512 = 3,539,456$ \\
    \textbf{Conv4b} & $4 \times 14 \times 14 \times 512$ & $512 \cdot 3 \cdot 3 \cdot 3 \cdot 512 + 512 = 7,078,400$ \\
    \textbf{Pool4} & $2 \times 7 \times 7 \times 512$ & 0 \\
    \textbf{Conv5a} & $2 \times 7 \times 7 \times 512$ & $512 \cdot 3 \cdot 3 \cdot 3 \cdot 512 + 512 = 7,078,400$ \\
    \textbf{Conv5b} & $2 \times 7 \times 7 \times 512$ & $512 \cdot 3 \cdot 3 \cdot 3 \cdot 512 + 512 = 7,078,400$ \\
    \textbf{Pool5} & $1 \times 4 \times 4 \times 512$ & 0 \\
    \textbf{Flattening Layer} & 8192 & 0 \\
    \textbf{fc6}& $4096$ & $8192 \cdot 4096 + 4096 = 33,558,528$ \\
    \textbf{fc7} & $4096$ & $4096 \cdot 4096 + 4096 = 16,781,312$ \\
    \textbf{softmax} & \#classes in dataset & $4096 \cdot \text{\#classes} + \text{\#classes}$ \\
    \end{tabularx}
    \caption{Output dimension and number of trainable parameters per layer in our \textit{C3D} model. The flattening layer collapses the feature maps in \textbf{pool5} into a one dimensional vector.}
    \label{tab:num_params}
\end{table}

The total number of parameters in the network adds up to $77,995,776$ plus the parameters residing in the last softmax output layer, whose amount depends on the number of output classes in the used dataset.
We report final results on the UCF-101 and Charades dataset.
The number of total parameters for these datasets are given in the following table \ref{tab:parameters_per_dataset}:
\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{l c c c} 
            Dataset & \#classes & \#Parameters (last layer) & \#Parameters (total) \\ 
            \hline
            UCF-101 & 101 & \hspace{0.5cm} $4096 \cdot 101 + 101 = 413,797$ \hspace{0.5cm} & $\textbf{78,409,573}$ \\
            Charades & 157 & $4096 \cdot 157 + 157 = 643,229$ & $\textbf{78,639,005}$ \\
    \end{tabularx}
    \caption{Total number of parameters in our implemented \textit{C3D} model according to the number of classes in the target dataset.}
    \label{tab:parameters_per_dataset}
\end{table}

As can be seen in table \ref{tab:parameters_per_dataset}, the number of total parameters is two orders of $10$ bigger than the number of parameters in the output layer.
It can be concluded, that most of the networks parameters, namely around $64\%$ are located in the last two fully-connected layers.
This is illustrated in figure \ref{fig:parameters_plot} and common in architectures with fully-connected layers, as they naturally contain more parameters than convolutional layers.
In recent image classification approaches with CNNs ??, the number of parameters is heavily reduces by substituting last fully connected layers.
We address this in section \ref{sec:future_work}.
We keep the architecture unchanged in order to compare our results, more precisely the effect of \textit{temporal order verification} on 3D convolutional networks, to the results of \textcite{tran_learning_2015} and \textcite{carreira_quo_2017}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_approach/parameters_plot}
    \caption{Distribution of parameters in our implementation of the \textit{C3D} model. Parameters in the last softmax layer are not included, because they depend on the number of output-classes in the target dataset.}
    \label{fig:parameters_plot}
\end{figure}


\subsubsection{Regularization Methods}
As around half of the model parameters are located in the last fully-connected layers, these are particularly prone to overfitting.
We therefore follow the original implementation of the \textit{C3D} model and incorporate Dropout\cite{srivastava_dropout:_2014} with a dropout rate of $0.5$ in those last two layers. 
Dropout has not been used in the convolutional layer, since the number of parameters in these layers is significantly smaller.
In order to provide regularization for the other parameters as well we incorporate L2 regularization \cite{ng_feature_2004}.
Batch Normalization \cite{ioffe_batch_2015-2} has been shown to be non-trivial to implement in a multi-GPU setup and incorporating it would have exceeded to scope of this work.
The original implementation of the \textit{C3D} model did not use it, \textcite{carreira_quo_2017} however implemented it a multi-GPU setup to compare their own approach with \textit{C3D}.

The main data-augmentation technique used in our experiments is random cropping of inputs (see section \ref{subsec:inputsampling}).


\subsection{Input Pipeline}
\label{subsec:inputpipeline}
A single network training step in our multi-GPU setup, i.e. processing an input batch and updating the model parameters as described in section \ref{sec:multi_gpu}, takes longer than preparing a single input batch.

This is due to the following reasons:
\begin{itemize}
    \item 
    Common sizes of action recognition video-datasets make it impossible to completely keep them in main memory.
    Therefore source videos need to be loaded from slower mass storage, in order to be processed further.
    \item 
    Videos are encoded to reduce their file-size and need to be decoded to obtain the frame-by-frame RGB-values.
    Pre-decoding would heavily increase the dataset size and exceed the storage limit of our hardware.
    \item
    Source videos usually have a higher spatial resolution than necessary as network input and need to be rescaled.
    \item
    It is common practise to perform data-augmentation, i.e. to artificially increase the number of unambiguous network inputs by manipulating the source data, usually through cropping, flipping or rescaling.
\end{itemize}

Therefore, even if the input-batch generation and model training would run in parallel in two different threads, the GPUs would not receive enough data to fully unlock the potential training speed-up of a multi-GPU setup.
To mitigate this problem, we implement a multi-threaded input pipeline, which can easily be scaled to more powerful CPUs (more processing cores) and is illustrated in figure \ref{fig:input_pipeline}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_approach/input_pipeline.eps}
    \caption{Illustration of the multi-threaded input pipeline. A \texttt{DataProvider} instance prepares input batches in parallel to provide enough data for the GPUs. Finished batches are enqueued asynchronously into a FIFO queue which then feeds the GPUs. Batches contain input video clips as well as training labels.}
    \label{fig:input_pipeline}
\end{figure}

An arbitrary number of threads can be created to asynchronically preprocess input batches.
The maximum reasonable number of threads depends on the number of CPU cores available on the target hardware.
Each thread has a reference to a single \texttt{DataProvider} instance, which encapsulates and provides the necessary pre-processing operations to produce an input batch in a single method (\texttt{get\_next\_training\_batch()}).
This method accesses a thread-lock to ensure synchronization between the threads, i.e. no thread will pre-process the same video during an epoch\footnote{An epoch describes a complete iteration through the training set.}.
As soon as the end of an epoch is reached, that is when one thread encounters the end of the training set, all other threads wait until the last batch is processed.
The training set is then randomly shuffled and the next epoch begins.

The batches get enqueued into a single FIFOQueue which resides in the main memory.
An input batch is then dequeued from the queue whenever necessary, i.e. after the previous training step completed on the GPUs.
Several kinds of queues as well as appropriate mechanisms to asynchronically enqueue and dequeue data are supported in the \textit{Tensorflow} framework \cite{abadi_tensorflow:_2016}.

The FIFOQueue represents as a buffer between the mass storage and the data-processing GPUs.
The currently enqueued number of batches thereby provides an accurate measure of the data flow, i.e. if more than one batch is enqueued during the overall training process, then the GPUs never had to wait for data.
Only in this case does the training process fully benefit from a multi-GPU setup.


\subsection{Multi-GPU Gradient Descent}
\label{sec:multi_gpu}
Incorporating multiple GPUs for parallel training of a deep neural network is not a trivial task and there are different ways to do so.
Parallelising network training with multiple GPUs can be divided into the following main approaches:
\begin{enumerate}
    \item Model Parallelism
    \item Data Parallelism
    \begin{enumerate}
        \item Asynchronous model updates
        \item Synchronous model updates
    \end{enumerate}
\end{enumerate}


\subsubsection{Model Parallelism}
Model parallelism describes dividing the to be trained network model into multiple parts and calculating each of these parts on a separate device.
Parallelizing a neural network that way turns out to be difficult, since the model needs to be split in a way to reduce cross-GPU communication as much as possible.
Particularly in fully-connected networks, splitting the model results in a lot of cross-device communication and generally no speed-up can be obtained from this approach.
The advantage of model parallelism therefore heavily depends on the network architecture itself.
Convolutional neural networks contain locally connected patches of neurons and are therefore easies to split, i.e. with less resulting cross-device communication.

The most intuitive way of splitting a network model horizontally, results in no parallelization speed-up at all, because the layers on GPU $n$ always have to wait for the outputs of GPU $n-1$.

Figure \ref{fig:model_parallelism} illustrates the above discussed alternatives to parallelize a single neural network model.

\begin{figure}[H]
    \begin{subfigure}[c]{\textwidth}
        \centering
        \includegraphics[height=5cm]{img_approach/model_parallelism1}
        \subcaption{Splitting a fully-connected network either layer-wise (\textit{middle}) or vertically (\textit{right}).}
        \vspace{0.5cm}
    \end{subfigure}
    \begin{subfigure}[c]{\textwidth}
        \centering
        \includegraphics[height=5cm]{img_approach/model_parallelism2}
        \subcaption{Splitting a partially-connected network vertically.} 
    \end{subfigure}
    \caption{Different configurations to parallelize a single neural network model across multiple GPUs \cite{geron_hands-machine_2017}}
    \label{fig:model_parallelism}
\end{figure}


\subsubsection{Data Parallelism}
Another way to use multiple GPUs for network training is to split the input batch rather than the network model, and process these sub-batches by replicated network models across multiple GPUs in parallel.
This approach is called data parallelism and each available GPU hosts an exact replica of the model that is to be trained.
An additional set of model weights is kept in the main memory to provide synchronization for the models before each new training step.
Figure \ref{fig:data_parallelism} provides an illustration of data parallelism in a multi-GPU setup.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img_approach/data_parallelism}
    \caption{General overview of multi-GPU training with data parallelism. \cite{geron_hands-machine_2017}}
    \label{fig:data_parallelism}
\end{figure}

Data parallelism hence yields additional computational overhead, because the shared model weights in the main memory need to be updated.
Since each model processes a sub-batch individually on its host GPUs, each model also calculates an individual set of weight updates, which needs to be communicated and merged with the weights in the main memory.
Since each GPU only processes a smaller sub-batch, the initial batch size can be increased.
This can be done in two ways: \textit{synchronically} or \textit{asynchronically}.

When updating the shared model weights \textit{synchronically}, the averaging step waits for all GPUs to finish the calculation of their individual gradients.
The averaged weight updates are then applied to the shared model weights, which are afterwards communicated back to the GPUs and thus synchronize the models to a homogeneous weight-state.
The models then proceed to process the next set of sub-batches.

Synchronous weight updates have the disadvantage, that the overall weight update has to wait for the slowest GPU to finish its computation.

Another option is to perform \textit{asynchronous} updates of the shared model parameters.
In that case, the gradients of a model are immediately used for updating as soon as a GPU finished its computation.
This averts the averaging delay and waiting for the slowest GPU.
However asynchronous updates yield the problem of ``stale'' gradients \cite{geron_hands-machine_2017}, i.e. whenever an update according to a GPU's gradients is issued, the shared weight may have been updated in the meantime and the current updates may not be beneficial anymore.

Benchmarking experiments conducted by Google Brain \cite{chen_revisiting_2016} indicate, that data parallelism with synchronous weight updates performs best.
Since our \textit{C3D} model contains locally connected convolutional layers as well as large fully-connected layers, it is difficult to split, i.e. to incorporate model parallelism for training.
Additionally, video-inputs are naturally bigger than images.
Since the amount of available memory on the GPUs limits the size of the input batches, training with data-parallelism is tempting.
We therefore decide to perform data parallel mini-batch gradient descent.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_approach/gradient_averaging.eps}
    \caption{Diagram of a single weight update step during data parallel synchronous gradient descent.}
    \label{fig:gradient_averaging}
\end{figure}


\subsection{Input Sampling}
\label{subsec:inputsampling}
Our \textit{C3D} model trains on input clips with a length of $16$ frames per step, yet most source videos provide more frames, i.e. have a longer temporal duration.
A sampling strategy to provide the network with properly sized inputs ($112 \times 112 \times 16$ pixels) is therefore needed.
This primarily includes temporal cropping, that is selecting a $16$ frame long sub-clip from the full temporal extend of the video.
We additionally incorporate spatial cropping: Source videos are initially rescaled to a larger spatial resolution and frames with the necessary width and height are cropped out to produce properly sized network inputs.


\subsubsection{UCF-101 Input-Clips}
Videos in the UCF-101 dataset are provided in a fixed resolution of $320 \times 240$ pixels (width $\times$ height) with an average length of $7.21s$ \cite{soomro_ucf101:_2012}.
Each video, in the training- as well as in the test-set, are annotated with a single action label per video.
During training we iterate over the list of training videos and sample one input clip for each of those training-videos.
The number of finished training epochs therefore corresponds to the overall number of processed input-clips per source video.

To reduce pre-processing time per training step, only a $16$ frame long time-interval is decoded from the source training video per sample.
For training on UCF-101, this interval is randomly sampled from the complete training video, since it only contains one single action.
The resulting $16$ frames are then rescaled to a fixed spatial resolution of $160 \times 120$ pixels, i.e. the spatial resolution is reduced by a factor of $2$.
The final network input is then cropped out by randomly selecting a $112 \times 112$ pixel wide region in the first frame and cropping the 15 following frames accordingly.

Following this procedure a multitude of different network inputs can be generated from a single source video.
We use a batch size of $40$ per training step, which results in $20$ clips per GPU during training.
This was found to be the biggest possible number of inputs to not exceed memory limitations of the available GPUs.

\subsubsection{Charades Input-Clips}
The videos in the Charades dataset are provided in multiple different resolutions.
To reduce the download time and dataset size, the $480p$-version of the dataset was downloaded, which contains videos with a maximal height of $480$ pixels.
In contrast to UCF-101 a single source video is annotated with multiple action instances, which possibly can overlap (see section \ref{subsec:charadeslabelling}).
To incorporate training on the Charades dataset in our implemented pre-processing pipeline, we treat each annotated action instance as a uniquely labelled video, from which input-clips can be sampled.

During training on the Charades dataset we iterate over the list of temporally localized action instances and sample one input-clip per instance and epoch.
To reduce pre-processing time, only a $16$ frame long interval is decoded from each action instance in the training set.
Analogously to the input sampling procedure on UCF-101 this interval is selected randomly.
Since the videos have different spatial resolutions, we rescale the shorter side of the previously sampled interval to $120$ pixels and 
spatially crop consecutive $112 \times 112$ pixel wide regions.

The batch size of 40 input-clips per step, i.e. 20 inputs per GPU is left unchanged for training on the Charades dataset.

\subsubsection{3D Temporal Order Verification}
\label{subsubsec:tovmethods}
Since obtaining the labels \textit{correct temporal order} and \textit{incorrect temporal order} requires no manual data labelling (see \ref{subsec:tov}), \textit{temporal order verification} can be seen as an unsupervised pre-training method.
Therefore also a large amount of unlabelled data can be used in the pre-training process, as long it contains videos of (preferably human) motion.

Promising candidate datasets are ActivityNet, because it contains a big number human actions, which unfortunately are noisily annotated and not temporally localised as well as Kinetics, which provides a big number of temporally localized actions.
These datasets however are only distributed via YouTube, i.e. each video needs to be downloaded individually.
At the time of this writing about half of the training-set of the Kinetics dataset, i.e. around $150,000$ videos were obtained.
To harvest the advantages of incorporating more data than the target dataset contains, the partially downloaded Kinetics videos were used for pre-training under the assumption that benefits will already be apparent and can be improved by incorporating the full dataset later-on.

Successful pre-training with \textit{temporal order verification} requires providing inputs, which are clearly distinguishable when permuted randomly as described by \textcite{misra_shuffle_2016}.
This means in practise for our \textit{C3D} model, that a sampled input clip needs to contain a certain amount of motion in order to be beneficial for pre-training.
A video clip with only a small magnitude of motion looks nearly the same, when permuted.

We investigate three different methods to transfer the sampling of positive and negative inputs from the original \textit{temporal order verification} approach as illustrated in figure \ref{fig:shufflelearn_approach} to our 3D convolutional network.
\bigskip


\textbf{Method 1}\\
A regular network input of $16$ frames is sampled from a video as described above.
The frames are then divided into three similarly sized chunks $a$, $b$ and $c$.
One of the chunks contains six video frames, which one is determined randomly.
Figure \ref{tov_sampling1} illustrates the approach with chunk $b$ containing 6 frames.

In order to provide a certain amount of motion in the sampled frames, the mean frames of each chunk are compared.
The difference between mean frames is used as a measure of change in the 16 frame long clip.
We empirically determine a threshold to filter out clips with a small amount of motion.

To generate a negative example (\textit{incorrect temporal order}) the order of chunks is being permuted.
For example: $(b, a, c)$ corresponds to a negative example.
To generate a positive example (\textit{correct temporal order}) the order of chunks is left unchanged.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img_approach/tov_sampling1}
    \caption{Grouping and permutation of input frames for generating positive and negative examples during pre-training of the \textit{C3D} model.}
    \label{fig:tov_sampling1}
\end{figure}


\textbf{Method 2}\\
We hypothesize that the previously described \textit{Method 1} may be flawed, since non-continuous frame transitions, i.e. cuts from permuting the frame chunks only occur at distinct frame regions.
A network trained to recognize these permutations could therefore overfit to just focus on these regions.

To alleviate this problem, another sampling method that allows cuts between every frame transition has been designed.
The authors of the original \textit{temporal order verification} approach \cite{misra_shuffle_2016} keep the first and last frame in place and only change the middle frame to generate a negative example in \textit{incorrect temporal order}.
To incorporate this, a number of frames $a$ in the beginning and end of the input clip is uniformly sampled between $1$ and $3$.
These frames are kept unchanged, as illustrated in figure \ref{fig:tov_sampling2}.

The remaining middle $16 - 2 \cdot a$ frames are divided into $n$ chunks, where $n$ is uniformly sampled between $2$ and $5$.
These $n$ chunks are then randomly permuted to create a negative example.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img_approach/tov_sampling2}
    \caption{Permutation of input frames to generate a possible cut between every input frame.}
    \label{fig:tov_sampling2}
\end{figure}


\textbf{Method 3}\\
A third approach we evaluate is most analogous to the original approach by \textcite{misra_shuffle_2016}.
Instead of feeding a single permuted clip into a single \textit{C3D} model, three input clips whose temporal relation amongst each other is permuted are fed into a triplet siamese \textit{C3D} model.
This triplet model is generated by incorporating three \textit{C3D} models with shared weights and no individual output layers.
The activations of the last fully-connected layers are concatenated and then classified by a final softmax output-layer into the two-classes \textit{correct temporal order} and \textit{incorrect temporal order}.

In total $64$ consecutive frames, i.e. four complete \textit{C3D} inputs are sampled from a training video.
In figure \ref{fig:tov_sampling3} these four $16$-frame inputs are denoted as $a$, $b$, $c$ and $d$.
To create a negative example the four individual inputs are left unchanged and their overall order is permuted before being fed in the triplet siamese network.

Specifically, a negative input can be generated by choosing either $(a,d,c)$ or $(b,a,d)$ as input for the network triplet, i.e. the first and last input are again in their correct place.
Positive inputs can be generated by choosing $(a,b,c)$ or $(b,c,d)$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img_approach/tov_sampling3}
    \caption{Pre-training with a triplet Siamese \textit{C3D} model equivalent to \cite{misra_shuffle_2016}}
    \label{fig:tov_sampling3}
\end{figure}
